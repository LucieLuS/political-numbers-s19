<!DOCTYPE html>
<html>
  <head>
    <title>Averages, Expectation, Aggregation</title>
    <meta charset="utf-8">
    <meta name="author" content="Understanding Political Numbers" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Averages, Expectation, Aggregation
### Understanding Political Numbers
### Feb 18, 2019

---







### Learning ggplot

&lt;img src="/Users/michaeldecrescenzo/Box Sync/teaching/270-numbers-S19/lectures/08-means/img/owl.jpg" width="60%" style="display: block; margin: auto;" /&gt;


---

# (Spooky voice) *Statistiiiiiiiics*

This week: "the signal and the noise"

- Today: Means
- Wednesday: Variance

In section: major tidyverse functions

Questions about exercise 1?


???

- elements of statistical reasoning
- data are noisy, how do we think about breaking through the noise to learn something?



---

## Major functions in the tidyverse

a.k.a. "verbs." They modify and return *data frames*

&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:left;"&gt; Function &lt;/th&gt;
   &lt;th style="text-align:left;"&gt; Operation &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; arrange() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Sort data frame along variable(s) &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; select() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Choose variables (columns) from a data frame &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; filter() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Choose cases (rows) from a data frame &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; mutate() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Create or modify variables &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; count() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Tabulate variable(s) in a data frame &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; summarize() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Calculate summary statistics from a data frame &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:left;"&gt; group_by() &lt;/td&gt;
   &lt;td style="text-align:left;"&gt; Implicitly partition a data frame along variable(s) &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

???


- Start with a data frame 
- Manipulate the data frame with function
- Result is a new data frame
- output of any verb can be the input to another verb
- string verbs together to make a data processing pipeline

Nice because the function names are easy to interpret



---

## Arrange


```r
# load tidyverse and gapminder data
library("tidyverse")
library("gapminder")

# With tidyverse verbs, the first argument is the data frame
# Sort by year and then by continent.
arrange(gapminder, year, continent)
```

```
## # A tibble: 1,704 x 6
##    country                  continent  year lifeExp     pop gdpPercap
##    &lt;fct&gt;                    &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;   &lt;int&gt;     &lt;dbl&gt;
##  1 Algeria                  Africa     1952    43.1 9279525     2449.
##  2 Angola                   Africa     1952    30.0 4232095     3521.
##  3 Benin                    Africa     1952    38.2 1738315     1063.
##  4 Botswana                 Africa     1952    47.6  442308      851.
##  5 Burkina Faso             Africa     1952    32.0 4469979      543.
##  6 Burundi                  Africa     1952    39.0 2445618      339.
##  7 Cameroon                 Africa     1952    38.5 5009067     1173.
##  8 Central African Republic Africa     1952    35.5 1291695     1071.
##  9 Chad                     Africa     1952    38.1 2682462     1179.
## 10 Comoros                  Africa     1952    40.7  153936     1103.
## # … with 1,694 more rows
```


---

# Select variables


```r
# Which variables do I want to keep?
# Again, first arg is the data frame name
# (notice lack of $)
select(gapminder, country, year, gdpPercap)
```

```
## # A tibble: 1,704 x 3
##    country      year gdpPercap
##    &lt;fct&gt;       &lt;int&gt;     &lt;dbl&gt;
##  1 Afghanistan  1952      779.
##  2 Afghanistan  1957      821.
##  3 Afghanistan  1962      853.
##  4 Afghanistan  1967      836.
##  5 Afghanistan  1972      740.
##  6 Afghanistan  1977      786.
##  7 Afghanistan  1982      978.
##  8 Afghanistan  1987      852.
##  9 Afghanistan  1992      649.
## 10 Afghanistan  1997      635.
## # … with 1,694 more rows
```


---

# Filter observations


```r
# Which cases (rows) do I want to keep?
# filter(dataset, logical test)
# keep rows where test result is TRUE
filter(gapminder, country == "United States")
```

.left-code[
Logical operators:

- `==` means "is equal to"
- `!=` means "not equal to"
- `&gt;` and `&lt;` mean "greater/less than"
- `&gt;=` and `&lt;=` are "greater than/less than or equal to"

Combine logical tests with `&amp;`  (and) or `|` (or)

- `filter(gapminder, country == "United States" &amp; year &gt; 2000)`
]

.right-plot[

```
## # A tibble: 12 x 6
##    country       continent  year lifeExp       pop gdpPercap
##    &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;     &lt;int&gt;     &lt;dbl&gt;
##  1 United States Americas   1952    68.4 157553000    13990.
##  2 United States Americas   1957    69.5 171984000    14847.
##  3 United States Americas   1962    70.2 186538000    16173.
##  4 United States Americas   1967    70.8 198712000    19530.
##  5 United States Americas   1972    71.3 209896000    21806.
##  6 United States Americas   1977    73.4 220239000    24073.
##  7 United States Americas   1982    74.6 232187835    25010.
##  8 United States Americas   1987    75.0 242803533    29884.
##  9 United States Americas   1992    76.1 256894189    32004.
## 10 United States Americas   1997    76.8 272911760    35767.
## 11 United States Americas   2002    77.3 287675526    39097.
## 12 United States Americas   2007    78.2 301139947    42952.
```
]

---

# Create variables with "mutate"


```r
# mutate(dataframe, new_variable = (whatever you want))
mutate(gapminder, 
       gdp = gdpPercap * pop)
```

```
## # A tibble: 1,704 x 7
##    country     continent  year lifeExp      pop gdpPercap          gdp
##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;        &lt;dbl&gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.  6567086330.
##  2 Afghanistan Asia       1957    30.3  9240934      821.  7585448670.
##  3 Afghanistan Asia       1962    32.0 10267083      853.  8758855797.
##  4 Afghanistan Asia       1967    34.0 11537966      836.  9648014150.
##  5 Afghanistan Asia       1972    36.1 13079460      740.  9678553274.
##  6 Afghanistan Asia       1977    38.4 14880372      786. 11697659231.
##  7 Afghanistan Asia       1982    39.9 12881816      978. 12598563401.
##  8 Afghanistan Asia       1987    40.8 13867957      852. 11820990309.
##  9 Afghanistan Asia       1992    41.7 16317921      649. 10595901589.
## 10 Afghanistan Asia       1997    41.8 22227415      635. 14121995875.
## # … with 1,694 more rows
```


---

# Count (or tabulate)


```r
# tabulate variable(s) with count().
# Again... result is a DATA FRAME
count(gapminder, continent, year)
```

```
## # A tibble: 60 x 3
##    continent  year     n
##    &lt;fct&gt;     &lt;int&gt; &lt;int&gt;
##  1 Africa     1952    52
##  2 Africa     1957    52
##  3 Africa     1962    52
##  4 Africa     1967    52
##  5 Africa     1972    52
##  6 Africa     1977    52
##  7 Africa     1982    52
##  8 Africa     1987    52
##  9 Africa     1992    52
## 10 Africa     1997    52
## # … with 50 more rows
```

???

I could save this result and use it to make a ggplot

- of counties per continent per year


---

# Summarize variables


```r
# New data frame of summary calculations
# Use na.rm = TRUE to skip missing values when calculating summary stats
summarize(gapminder, 
          mean_lifeexp = mean(lifeExp),
          min_lifeexp = min(lifeExp),
          max_lifeexp = max(lifeExp, na.rm = TRUE))
```

```
## # A tibble: 1 x 3
##   mean_lifeexp min_lifeexp max_lifeexp
##          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;
## 1         59.5        23.6        82.6
```

---

# Group data by variables


```r
# partition data into groups. Pretty benign when used alone
group_by(gapminder, continent) 
```

```
## # A tibble: 1,704 x 6
## # Groups:   continent [5]
##    country     continent  year lifeExp      pop gdpPercap
##    &lt;fct&gt;       &lt;fct&gt;     &lt;int&gt;   &lt;dbl&gt;    &lt;int&gt;     &lt;dbl&gt;
##  1 Afghanistan Asia       1952    28.8  8425333      779.
##  2 Afghanistan Asia       1957    30.3  9240934      821.
##  3 Afghanistan Asia       1962    32.0 10267083      853.
##  4 Afghanistan Asia       1967    34.0 11537966      836.
##  5 Afghanistan Asia       1972    36.1 13079460      740.
##  6 Afghanistan Asia       1977    38.4 14880372      786.
##  7 Afghanistan Asia       1982    39.9 12881816      978.
##  8 Afghanistan Asia       1987    40.8 13867957      852.
##  9 Afghanistan Asia       1992    41.7 16317921      649.
## 10 Afghanistan Asia       1997    41.8 22227415      635.
## # … with 1,694 more rows
```


---

# Group and summarize

.pull-left[

```r
# the `&lt;-` scans the next line
gap_by_continent &lt;- 
  group_by(gapminder, continent) 

summarize(gap_by_continent, 
          mean_life = mean(lifeExp))
```

```
## # A tibble: 5 x 2
##   continent mean_life
##   &lt;fct&gt;         &lt;dbl&gt;
## 1 Africa         48.9
## 2 Americas       64.7
## 3 Asia           60.1
## 4 Europe         71.9
## 5 Oceania        74.3
```
]


.pull-right[

```r
# Because result of group_by() is a data frame,
#  you could pass result directly to summarize
summarize(group_by(gapminder, continent),
          mean_life = mean(lifeExp))
```

```
## # A tibble: 5 x 2
##   continent mean_life
##   &lt;fct&gt;         &lt;dbl&gt;
## 1 Africa         48.9
## 2 Americas       64.7
## 3 Asia           60.1
## 4 Europe         71.9
## 5 Oceania        74.3
```
]



---

class: inverse, middle, center

# Averages


---

### Question: do women vote for Democrats more than men do?

.left-code[
Break the question down: 

1. What's the *average* rate of Democratic voting among women?
2. Among men?
3. How different are they?
]


???

Not just asking one man and one woman

- not just asking my friends

Statistics is important to assess the strength of the difference

- data are noisy, as you can see by all the wobbling around
- maybe these differences are actually random?

--

.right-plot[





&lt;img src="08-means_files/figure-html/plot-gender-gap-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]



---

### Question: is voter turnout higher among older voters?






.left-code[
As a comparison of averages: 

- Average turnout among older voters
- Among younger voters
- with a twist: age is continuous(ish)
]



???

Can't just compare young to old

- there is a smooth transition from young to old

Does the average level of turnout *increase* as age increases

Stability of the pattern

- Signal and noise: if noise is *random* then it cancels out when averaging 
- Think about it: if `\((n)\)`-year olds are just a little different from `\((n+1)\)`-year-olds, then you shouldn't be surprised to find only slight differences from one year to the next
- over time these differences accumulate, but comparing 29 year olds to 30  year olds, not that different



--



.right-plot[
&lt;img src="08-means_files/figure-html/plot-age-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]




---

class: middle, center

# Averages are useful because they tell us about the *typical* behavior in the data

Practically &amp; Ethically: individuals ≠ averages


???

statistics, very interested in the average

- there's variation around the average, but we want to know the *tendency*

Ethically this isn't always super simple

- people aren't average
- putting too much emphasis on somebody's group characteristics will erase their individuality
- a lot of the time, the grouping that you're looking at doesn't explain most of why somebody does something




---

### Averaging (the math)



`\(x = \begin{bmatrix} 6\quad15\quad8\quad16\quad17\end{bmatrix}\)`.

The average of `\(x\)`, we call `\(\bar{x}\)`.

--

`\begin{align}
  \bar{x} &amp;= 
    \frac{\sum\limits_{i = 1}^{n} x_{i}}{n} \\[18pt]
  \bar{x} &amp;= 
    \frac{1}{n}\sum\limits_{i}^{n} x_{i}
\end{align}`



???

Factor out the 1/N



--

.pull-left[

```r
mean(x)
```

```
## [1] 12.4
```
]
.pull-right[

```r
sum(x) / length(x)
```

```
## [1] 12.4
```
]


???

Just to reassure you that this is doing what you think it's doing


---

## Strategies for averaging different data types

???

Some data types are numeric and some are not

- how do we calculate averages of non-numeric data types

--

.pull-left[
Quantitative (interval and ratio) data


```r
summarize(gapminder, 
          avg_lifeexp = mean(lifeExp), 
          avg_gdpPercap = mean(gdpPercap))
```

```
## # A tibble: 1 x 2
##   avg_lifeexp avg_gdpPercap
##         &lt;dbl&gt;         &lt;dbl&gt;
## 1        59.5         7215.
```
]

--

.pull-right[
Categorical (nominal and ordinal) data


```r
# Proportion of data in continents.
summarize(gapminder,
          pr_afr = mean(continent == "Africa"),
          pr_euro = mean(continent == "Europe"))
```

```
## # A tibble: 1 x 2
##   pr_afr pr_euro
##    &lt;dbl&gt;   &lt;dbl&gt;
## 1  0.366   0.211
```

**If we have a vector of 1s and 0s ("successes" and "failures"), the mean is equal to the proportion of 1s (successes)**

]


???

- proportions are analogous to averages
- If we designate one category as a "1" and another category as a "0", the proportion is the mean



---

### Why we like averages: noise canceling out

.pull-left[
We flip a coin. 


```r
# make a coin vector
coin &lt;- c("Heads", "Tails")

# "flip" the coin
sample(coin, 1)
```

```
## [1] "Heads"
```
]

--

.pull-right[
We flip it 5 times.


```r
# 'replace' means we put the coin back each time
flips &lt;- sample(coin, 5, replace = TRUE)

# what's the proportion of heads?
mean(flips == "Heads")
```

```
## [1] 0.2
```
]


---

### Why we like averages: noise canceling out


.left-code[
Flip 100 times. After each flip, find proportion of heads *up to that point*

Eventually this "running average" should approach what number?


```
## # A tibble: 100 x 3
##    trial flip  running_mean
##    &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;
##  1     1 Heads        1    
##  2     2 Tails        0.5  
##  3     3 Heads        0.667
##  4     4 Tails        0.5  
##  5     5 Tails        0.4  
##  6     6 Heads        0.5  
##  7     7 Tails        0.429
##  8     8 Tails        0.375
##  9     9 Tails        0.333
## 10    10 Heads        0.4  
## # … with 90 more rows
```

]

--

.right-plot[
&lt;img src="08-means_files/figure-html/unnamed-chunk-16-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]



---

class: inverse, middle, center


# Expectation


---

## Expectation


The true / theoretical / long-run average


???

Suppose I had a random process that produced data

- maybe a random process that determines how tall a person is
- If I played that process out forever and created an infinite number of people
- what's the long-run average height?


--

### Example: more coins

Suppose that the variable `\(\mathbf{X}\)` contains an *arbitrary number* of coin flips (1 = "Heads", 0 = "Tails").

As the number of trials approaches `\(+\infty\)`, the mean of `\(\mathbf{X}\)` approaches what value?

--

`\begin{align}
  \mathrm{E}[\mathbf{X}] = 0.5
\end{align}`

Probabilities are one type of *expectation*, but not the only kind

???

You have a 50% change of a 1, 50% change of a 0, so average those things out, you get 0.5

---

## Another toy example: rolling a die

Roll a six-sided die. What's the expected value?
--

It's **3.5**, why?

--

It's the "theoretical average."

???

Buckle up folks

- not for memorizing
- just to get an intuition

--

`\begin{align}
  \mathrm{E}[X] &amp;= \sum\limits_{k = 1}^{K} x_{k}p_{k} \\
  &amp;= x_{1}p_{1} + x_{2}p_{2} + \ldots + x_{K}p_{K}
\end{align}`

- `\(x_{k}\)` represents a possible outcome
- `\(p_{k}\)` is the probability of outcome `\(k\)`
- `\(K\)` is the total number of possibilities

--

`\begin{align}
  \mathrm{E}[\mathrm{Die}] = \left(1 \times \frac{1}{6} \right) + \left(2 \times \frac{1}{6} \right)  + \ldots + \left(6 \times \frac{1}{6} \right) = 3.5
\end{align}`

Expectation is a **weighted average of all possible outcomes** (each outcome weighted by its probability of occurrence)

---

## Why does this matter?

The *theoretical average* influences the data we collect, but our data don't *perfectly reflect* the true expectation

--

.left-code[
- Candidate A and B are running for Senate
- TRUE support for Candidate A is 54%
- "the population parameter" ( `\(\mu = 0.54\)` )
]

--

.right-plot[
We take a survey of 500 voters




```r
# Voters support A or B randomly with given probabilities.
# repeat 500x with replacement
voters &lt;- sample(c("A", "B"), prob = c(0.54, 1 - 0.54),
                 size = 500, replace = TRUE)

# what proportion of the sample is voting for A?
mean(voters == "A")
```

```
## [1] 0.56
```
]

--

**Expected value** (a.k.a. "population mean") is `\(\mu = 0.54\)`

**Estimated value** (a.k.a. "sample mean") is `\(\bar{x} = 0.56\)`


???

You can never count on your data to be a 100% perfect reflection of the *real universal truth*

- Process that create data are *noisy*, they are affected by random chance
- The reason why we do statistics is to deal with the uncertainty in estimating the real world



---

## Taking a sample

.left-code[
We will never know the *true mean* ( `\(\mu\)` ) with certainty.

But we can take samples of data and calculate the *sample mean* ( `\(\bar{x}\)` ) within the sample.
]

--

.right-plot[
&lt;img src="08-means_files/figure-html/replicates-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]


???

- Each of these polls has the same "theoretical" accuracy
- The only reason any of them is different is random chance
- Some people answered the phone, other people didn't
- Sometimes we're really close to the truth, sometimes we are really far
- In the real world, we don't have the truth to compare to, nor do we have lots of datasets to compare side by side. We only have one data set. And we are unsure how accurate the data are.


---

class: center, middle

# The whole point of statistics 

## is to figure out how confident we can be about the *real truth*

###  given that we only can observe our imperfect sample of data


???

- Statistics gives us rules of probability
- that we use to determine how *wrong* we can expect to be, given that I have a sample of 500
- As we get more data, our expected amount of error goes down
- And this all obeys nice mathematical rules


---

class: inverse, middle, center

# Aggregation




---

## Aggregation

We have data at some level of analysis, and we want to summarize it at a higher level of analysis

--

.pull-left[
Life expectancy *aggregated* by year


```
## # A tibble: 12 x 2
##     year lifeExp_mean
##    &lt;int&gt;        &lt;dbl&gt;
##  1  1952         49.1
##  2  1957         51.5
##  3  1962         53.6
##  4  1967         55.7
##  5  1972         57.6
##  6  1977         59.6
##  7  1982         61.5
##  8  1987         63.2
##  9  1992         64.2
## 10  1997         65.0
## 11  2002         65.7
## 12  2007         67.0
```
]

--

.pull-right[
Life expectancy *aggregated* by continent-year


```
## # A tibble: 60 x 3
## # Groups:   continent [5]
##    continent  year lifeExp_mean
##    &lt;fct&gt;     &lt;int&gt;        &lt;dbl&gt;
##  1 Africa     1952         39.1
##  2 Africa     1957         41.3
##  3 Africa     1962         43.3
##  4 Africa     1967         45.3
##  5 Africa     1972         47.5
##  6 Africa     1977         49.6
##  7 Africa     1982         51.6
##  8 Africa     1987         53.3
##  9 Africa     1992         53.6
## 10 Africa     1997         53.6
## # … with 50 more rows
```
]


???

With averaging, we take all our data and create one summary measure

- We often refer to "aggregation" as summarizing but within groups of data
  - group is year
  - group is continent-year
- Aggregating is taking all the observations in the group and summarize them with *one value*
  - in this case it's the mean
  - I could have also calculated min, max, median, and so on
  - but the point is there is only *one* mean per group, *one* median per group



---

### "Conditional" averages and "conditional" probabilities

&lt;img src="/Users/michaeldecrescenzo/Box Sync/teaching/270-numbers-S19/lectures/08-means/img/conditional_risk.png" width="55%" style="display: block; margin: auto;" /&gt;

???

Averages / probabilities within a group

Jargon: "conditional on" group membership


---

class: middle

.left-column[
**Ecological Fallacy:** Assuming that group-level patterns apply to individuals within the group
]

.right-column[
&lt;img src="/Users/michaeldecrescenzo/Box Sync/teaching/270-numbers-S19/lectures/08-means/img/ecological.png" width="65%" style="display: block; margin: auto;" /&gt;
]




---

class: inverse, middle, center


# See ya


### Wednesday is (spooky voice) randomnessssssss
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="https://platform.twitter.com/widgets.js"></script>
<script>var slideshow = remark.create({
"ratio": "16:9",
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function() {
  var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})();</script>

<script>
(function() {
  var i, text, code, codes = document.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
})();
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
