---
title: "Statistical Significance"
# subtitle: '(Estimating Linear Relationships)'
author: "Understanding Political Numbers"
date: "March 6, 2019"
output:
  xaringan::moon_reader:
    lib_dir: libs
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    # mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_SVG"
    css: xaringan-themer.css
    nature:
      ratio: "16:9"
      highlightStyle: default
      highlightLines: true
      countIncrementalSlides: false
      beforeInit: "https://platform.twitter.com/widgets.js"
seal: false
---

class: inverse, middle, center

# Review


```{r setup-rmd, eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE}

# rmarkdown::render(here::here("lectures", "13-significance", "13-significance.Rmd"))
# knitr::purl(here::here("lectures", "13-significance", "13-significance.Rmd"))

source(here::here("R", "setup-lectures.R"))

# They're good DAGs, Brent
# library("dagitty")
# library("ggdag")

# box
# library("boxr"); box_auth()

# library("viridis")
# library(png)
# library(grid)
# library(gridExtra)

options(scipen = 99999)


hook_source <- knitr::knit_hooks$get('source')
knitr::knit_hooks$set(source = function(x, options) {
  x <- stringr::str_replace(x, "^[[:blank:]]?([^*].+?)[[:blank:]]*#<<[[:blank:]]*$", "*\\1")
  hook_source(x, options)
})

# chunks:
# hide code and messages
# cache everything
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, 
                      warning = FALSE, message = FALSE,
                      cache = TRUE, 
                      cache.path = here::here("lectures", "cache", "13_"),
                      fig.align = "center", # eval.after = 'fig.cap',
                      fig.retina = 3 # , dpi = 100
                      )

img <- "lectures/13-significance/img"
```


???

Why regression?

- we are trying to understand the social world
- how do social things happen
- this means cause and effect
- in math: cause and effect means x causes y
- If x causes y, they should be related
- if I change x, y would change as a result
- so that's why we care about relationships
- we care about regression because we want to understand relationships
- so we need a way to estimate the relationship


**RESIST TEMPTATION TO WRITE EVERYTHING**


---

.pull-left[

### Regression Review

1. **Assume: $\mathrm{E}[Y \mid X]$ is a line**

   Expected average $y$, conditional on its $x$ value

3. ** $\hat{y}_{i}$ is predicted $y$  for observation $i$. **
   
   $\hat{y}_{i} = a + bx_{i}$

2. ** $y_i$ is the observed $y$ (prediction + residual error)**

   $y_{i} = a + bx_{i} + e_{i}$

4. **Residual error: actual minus predicted <br>**
   
   $e_{i} = y_{i} - \hat{y}_{i}$

5. **"Ordinary least Squares" (OLS) estimation: pick $a$ and $b$ that minimize error**

   Technically, minimizing the "sum of squared error"

]


.pull-right[
```{r review-code, echo = TRUE}
library("tidyverse") # contains 'midwest' data

lm(percprof ~ percollege, data = midwest)
```


```{r review-plot, fig.width = 5, fig.height = 3.5, out.width = "100%", results = 'hide'}
edreg <- lm(percprof ~ percollege, data = midwest)

edcoefs <- edreg %>%
  coef() %>%
  round(2) %>%
  set_names(c("a", "b")) %>%
  print()

revplot <- ggplot(data = midwest, 
       aes(x = percollege, y = percprof)) +
  geom_point() +
  labs(title = "Education in Midwest Counties",
       x = "Percent w/ College Degree",
       y = "Percent w/ Prof. Degree") 

revplot +
  geom_smooth(method = "lm") +
  annotate(
    geom = "text", x = 15, y = 14, 
    label = TeX('$\\hat{y} = -1.79 + 0.34x$')
  ) +
  NULL
```

]

---


.pull-left[
### Warnings

1. Beware: Does a linear relationship make sense

2. Beware: extrapolation beyond data (top figure)

3. Beware: patterns in residuals (bottom figure)

4. Beware: influential outliers

]


.pull-right[
```{r, fig.width = 5, fig.height = 3.5, out.width = "90%", results = 'hide'}
revplot +
  geom_rect(aes(xmin = 0, xmax = 100,
                ymin = 0, ymax = 100),
            color = "gray", fill = NA,
            linetype = "dashed") +
  coord_cartesian(xlim = c(-50, 100),
                  ylim = c(-50, 100)) +
  geom_abline(intercept = edcoefs['a'],
              slope = edcoefs['b'],
              color = "blue") +
  labs(subtitle = "Axes Expanded")

resplot <- edreg %>%
  broom::augment() %>%
  ggplot(aes(y = .resid, x = .fitted)) +
    geom_hline(yintercept = 0, color = "gray") +
    geom_point() +
    labs(title = "Residuals vs. Fitted Values",
         x = TeX("Predicted (fitted) value $\\left(\\hat{y}\\right)$"),
         y = "Residual Error (e)") 

resplot +
  geom_smooth(se = FALSE, color = "red")
```
]




---


### Assumptions about leftover error

.pull-left[

We assume that error $e_{i}$ is random noise

- \*\**After* accounting for $x$

- Only $x$ affects $y$? No.

- $e_{i}$ is the sum of "everything else"

- Accumulation of random noise $\rightarrow$ normal distribution

- Expected value of error is 0

]


.pull-right[

```{r, fig.width = 5, fig.height = 3.5, out.width = "100%", results = 'hide'}
edreg %>%
  broom::augment() %>%
  ggplot(aes(x = .std.resid)) +
    geom_histogram(bins = 20, aes(y = ..density..),
                   color = "gray50", fill = "gray90") +
    stat_function(fun = dnorm, color = "red", size = 1) +
    coord_cartesian(xlim = c(-6, 6)) +
    labs(x = "Residual (scaled)", y = "Density")
```

]





---

class: inverse, middle, center

# Statistical Significance




---

class: center, middle

### A result is *statistically significant* if is was unlikely to have occurred by chance

???

- We look at the relationship of $x$ and $y$, we will find some slope.
- Is this just caused by noise in our data
- We compare the average test score between two groups. We find some difference.
- Is this difference real, or just caused by noise in our data?



---

# The "True" Model

We estimate $a$ and $b$, but estimates are noisy. What can we learn about the *true* equation?

--

.pull-left[
#### The true equation

\begin{align}
  y_{i} &= \alpha + \beta x_{i} + \epsilon_{i}
\end{align}

Problem: We don't know $\alpha$ and $\beta$ and never will

]

--

.pull-right[

#### The estimated equation

\begin{align}
  y_{i} &= a + bx_{i} + e_{i}
\end{align}

$a$ and $b$ are imperfect estimates of $\alpha$ and $\beta$

]

--


<br><br>

#### **Statistical inference** is "what conclusions can I draw about $\beta$ even though I can't see it?"



???

Even if $\beta = 0$ , we will find $b \neq 0$



---

class: center, middle

### A result is *statistically significant* if is was unlikely to have occurred by chance


**We want to make inferences about the "true" parameters, but we only observe a sample of data.**


---


.pull-left[

```{r null-data}
n <- 30

set.seed(321)
test_data <- 
  tibble(x = rnorm(n),
         y = rnorm(n))
```


```{r null-scatter, results = 'hide', fig.show = 'asis', fig.height = 4, fig.width = 4, out.width = "100%"}
ggplot(test_data, aes(x, y)) +
  geom_abline(slope = 0, linetype = 2, color = "gray") +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  coord_cartesian(ylim = c(-3, 3),
                  xlim = c(-3, 3)) +
  labs(title = "Relationship? Or Random?",
       subtitle = TeX("$\\beta = ?$"))

```
]

--

.pull-right[
### The "null hypothesis"

Assume that $\beta = 0$

Estimate the model on data

```{r null-reg, echo = TRUE}
ex_reg <- lm(y ~ x, data = test_data) %>%
  print()
```

**Assuming that $\beta = 0$, what's the probability $(p)$ of observing a $b$ this big *by random chance*?**
]

???

if $p$ is low enough: statistically significant





---

class: center, middle

### A result is *statistically significant* if is was unlikely to have occurred by chance

**We want to make inferences about the "true" parameters, but we only observe a sample of data.**

**What's the *probability* of observing our slope, *if the null were true* ( $p$ value )**


???

Calculate the $p$-value, but where do they come from?



---

### Find the $p$-value

** $p$ value: ** The probability of observing a slope *at least this big* if the null hypothesis is true

--

.left-code[

Output from `tidy()` (from the `broom` package)

- a data frame!
- `estimate`: coefficients ( $a$ and $b$ values )
- `std.error`: uncertainty of estimates
- `statistic`: standardized slope (estimate / std.err)
- `p-value`: self-explanatory

]


.right-plot[
```{r summary, echo = TRUE}
# 'broom' pkg for model output
# install.packages("broom")

# load it
library("broom") 

# info about model estimates
tidy(ex_reg)
```
]




---

## "Rejecting the null hypothesis"

.pull-left[

Null hypothesis significance testing:

- "Assuming the null hypothesis is true, the probability of observing a slope at least this *extreme* is ( $p$ )"

- If $p$ is really low, then it's unlikely that the data come from the null hypothesis

- "Statistical significance" means $p$ is lower than some threshold

- Reject the null hypothesis at $(1-p)$% confidence
]

--

.pull-right[


$p < 0.1$: significant at the 10% level (reject the null with 90% confidence)

$p < 0.05$: significant at the 5% level (reject the null with 95% confidence)

$p < 0.01$: significant at the 1% level (reject the null with 99% confidence)


**Lower $p$ values, stronger signal, more confident that $\beta \neq 0$**

]



---

class: center, middle

### A result is *statistically significant* if is was unlikely to have occurred by chance

**We want to make inferences about the "true" parameters, but we only observe a sample of data.**

**What's the *probability* of observing our slope, *if the null were true***

**An estimate is *significant* if the probability of getting it, under the null, is "sufficiently low"**




---

### Where do $p$-values come from? 

.pull-left[
Let's do a `S I M U L A T I O N`

- Generate 10k datasets containing $x$ and $y$

- In every dataset, the **true slope** is zero

- In every dataset, our **estimated slope** is not zero (thanks to random error $e_{i}$)

]


```{r simulate, results = 'hide', fig.show = 'asis'}
n_trials <- 10000

sims <- tibble(trial = 1:n_trials) %>%
  group_by(trial) %>%
  mutate(data = map(1, ~ tibble(x = rnorm(n), y = rnorm(n))),
         lm = map(data, ~ lm(y ~ x, data = .x)),
         tidy = map(lm, tidy, conf.int = TRUE),
         b = map(lm, ~ tidy(.x) %>%
                       filter(term == "x") %>%
                       pull(statistic))) %>%
  unnest(b) %>%
  print()
```

--

.pull-right[

```{r plot-nulls, fig.width = 5, fig.height = 4, out.width = "100%"}
ggplot(sims, aes(x = b)) +
    geom_histogram(binwidth = .1, aes(y = ..density..),
                   color = "black", fill = "gray90",
                   boundary = TRUE) +
    geom_line(data = tibble(x = seq(-5, 5, .001)),
              aes(x = x, y = dt(x, df = 100 - 2)),
              color = "maroon",
              size = 1) +
    coord_cartesian(xlim = c(-5, 5)) +
    labs(x = "Estimated slope (b)",
         y = NULL,
         title = "Distribution of Estimated Slopes",
         subtitle = TeX("True $\\beta = 0$"))
```

**We know the theoretical distribution of "by-chance" slopes**

]





---

### We know the distribution of "by-chance" slopes

.left-code[
Compare slopes by *standardizing* them: $t = \dfrac{b}{std.err(b)}$. 

"Big" $t$ values are unlikely

$p$ value is the probability of getting an even "bigger" $t$ value
]

.right-plot[

```{r, out.width = "100%"}
include_graphics(here(img, "zscores.png"))
```
]


???

We know what to expect, if the relationship is null



---

## Confidence levels and $p$-values

.pull-left[

95% Interval = $b \pm 1.96(se(b))$

Naive interpretation: 95% chance that the true value is within the interval

Better interpretation: The parameter is in the interval or it's not. The interval contains the true value in 95% of samples (if you could take an infinite number of samples, which, you can't)

Practical interpretation: Interval contains all the values I can't reject. if it doesn't contain zero, you can reject zero
]

.pull-right[

```{r confints, out.width = "100%", fig.width = 6, fig.height = 4}
cigg <- sims %>%
  unnest(tidy) %>%
  filter(term == "x") %>%
  filter(trial <= 100) %>%
  ggplot(aes(x = trial, y = estimate)) 

cigg +
  geom_hline(yintercept = 0) +
  geom_pointrange(aes(ymin = conf.low, ymax = conf.high,
                      color = sign(conf.low) == sign(conf.high),
                      fill = sign(conf.low) == sign(conf.high)),
                  show.legend = FALSE,
                  shape = 21) +
  scale_color_manual(values = c("black", "red")) +
  scale_fill_manual(values = c("white", "red"))
```
]





---

## Inference issues with $p$ values

--

Null hypothesis testing: Higher quality learning by rejecting inconsistent ideas (*falsifying* the null? Probabilistically?)

--

If we want to be 95% confident, 5% of the "null models" will appear significant

--

It takes *lots* of data to estimate small effects w/ statistical significance

--

Insignificance does *not* mean "no relationship," only that there wasn't enough data to reject the null hypothesis

--

Relationships are everywhere, we just need enough data to make confident inferences about what they are




---

class: center, middle

### A result is *statistically significant* if is was unlikely to have occurred by chance

**We want to make inferences about the "true" parameters, but we only observe a sample of data.**

**What's the *probability* of observing our slope, *if the null were true***

**An estimate is *significant* if the probability of getting it, under the null, is "sufficiently low"**

**Null relationships can still "pop" as significant, and "non-null" relationships may fail to show insignificance**


<!-- MICHAEL's email 

- Grades are really good, better than essay 1 maybe
- Michael will have left feedback on your code. They will be skewed toward comments on what could be better, but this is just for transparency. Even though your comments may only highlight stuff that could have been better, doesn't mean you did a bad job. 

Common areas:

- new object after mutate()
- names(), and for diagnosing issues with the above ^
- mean in each group means group_by() %>% summarize()
- Y-axis of a histogram
- Don't just pipe everything together

Style:

- Include comments
- break commands across multiple lines (ggplot)
- Space out code (spaces between operators, and between lines)
- Don't put View() in the script
- Style guide links?

-->